defaults:
  - base_trainer

gpus: 1
min_epochs: 5
max_epochs: 20
deterministic: false
precision: 32

# if plugins= "ddp_sharded" then model-parallel training is possible (accelerator has to be ddp then)
plugins: null

# Choose whether to use a distributed backend (For multi-GPU training) or not:
accelerator: null # [null, "ddp", "ddp2", "ddp_cpu"]

# interval for logging metrices:
log_every_n_steps: 10
