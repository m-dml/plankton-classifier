# Default Configuration file for setting up experiments for GanCasting. Do not change!
# New config files do not have to include all of these entries. Only provided entries will be overwritten.
# Only keys that are defined in the "default config" file are allowed to be set.
# The "None" value in python is in yaml called "null" without "".
this_config_file: "default_config.yaml"

# --------------------------------------------------------------------------------------
# ==================
# technical:
# ==================

experiment_name: Resnet_classifier_plankton
batch_size: 16
min_epochs: 1
max_epochs: 5
learning_rate: 0.002

# makes the model completely reproducible but slower:
deterministic_trainer: false

random_seed: 42

# For debugging and developing:
debug_mode: false
fast_dev_run: false

# if precision=16 then mixed precision is used:
precision: 32

# if plugins= "ddp_sharded" then model-parallel training is possible (accelerator has to be ddp then)
plugins: null

# Choose whether to use a distributed backend (For multi-GPU training) or not:
accelerator: null  # [null, "ddp", "ddp2", "ddp_cpu"]

# CPUs for the dataloader. For windows this should be 0, otherwise it will crash:
# Also with streaming data this should be 0:
num_workers: 20

# Number of GPUs per Node (On strand max. is 1). If 0 then use CPUs:
gpus: 0

# When this is changed, the the slurm script has to be adjusted as well:
num_nodes: 1

# interval for logging metrices:
log_interval: 10

# weather to weight the losses according to the number of samples per class
use_weighted_loss: false

# whether to download a pretrained network
use_pretrained: false

# --------------------------------------------------------------------------------------
# ==================
# Paths:
# ==================

# top path where to find all plankton data:
plankton_data_base_path: "/gpfs/work/machnitz/plankton_dataset"

# The folder structure is Sorted/ClassName/DateFolder/*.png
new_sorted_plankton_data: "new_data/4David/M160/Sorted"

# Contains unsorted data in date-sorted subfolders. Files end on .png
new_unsorted_plankton_data: "new_data/4David/HE570/rois/20*/20*"

# the data in this dataset is grouped, meaning each class (folder) may contain subclasses (another folder). Files
# end on .tif:
old_sorted_plankton_data: "VPR_M87_grouped"

# --------------------------------------------------------------------------------------
# ==================
# DataLoader:
# ==================

# Bool whether to use the old data for training:
use_old_data: false

# Bool whether to use the new data for training. Either old, new or both have to be used:
use_new_data: true

# use to combine come classes into super-classes
# if you don't want to use super-classes set as null

super_classes:
  MarineSnow:
    - MarineSnow_Comet
    - MarineSnow_Detritus
    - MarineSnow_FaecalPellet
    - MarineSnow_Jelly
  Radiolaria:
    - Radiolaria
    - Foraminifera

# whether to oversample underrepresented data:
oversample_data: false

# whether to use finer distinctions of different object types as different target categories
# this has not effect on datasets with a flat directory structure
use_subclasses: false

# whether to load the whole dataset into memory at the beginning of the training:
preload_dataset: false

excluded_classes: ["Blurry", "Bubbles"]

# Relative splitting of the data into subsets. Splitting is done along the time dimension and data is not shuffled
# before the splitting, so that the validation data always comes chronologically after the train data.
# The size of the test dataset is the 1 - (train_split + validation_split):
train_split: 0.7
validation_split: 0.2

log_confusion_matrices: True  # whether to log confusion matrices on every batch
log_images: False  # whether to log images and labels for first batch of every validation and training epoch

# Whether the different datasets should be shuffled or not:
shuffle_train_dataset: true
shuffle_validation_dataset: false
shuffle_test_dataset: false
tensorboard_logger_logdir: "logs/tb_logs/"
checkpoint_file_path: "logs/checkpoints"

# The size to which images should be padded/cropped
final_image_size: 128

# All transforms specified below are being composed and applied to the images. Where necessary, supply CONFIG.final_image_size as an input
transform:
  - "SquarePad()"
  - "transforms.Resize(size=[CONFIG.final_image_size, CONFIG.final_image_size])"
  - "transforms.RandomRotation(360)"
  - "transforms.RandomHorizontalFlip()"
  - "transforms.RandomVerticalFlip()"
  - "transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)"
  - "transforms.ToTensor()"
