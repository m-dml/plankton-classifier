{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0fb019",
   "metadata": {},
   "source": [
    "This notebook serves the same function as main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf36a989",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "config_file = '../dg_config.yaml'\n",
    "\n",
    "max_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed6bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import yaml\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.models.LightningBaseModel import LightningModel\n",
    "from src.utils import CONFIG\n",
    "from src.utils.DataLoader import PlanktonDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d97dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    with open(os.path.abspath(config_file), \"r\") as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "        CONFIG.update(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22c3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config()\n",
    "\n",
    "# log directory is one level up since we're in the notebooks directory\n",
    "CONFIG.tensorboard_logger_logdir = os.path.join('..', CONFIG.tensorboard_logger_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4ea796",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "root <module> WARNING {'new_config_class': False, 'this_config_file': 'default_config.yaml', 'experiment_name': 'Resnet_classifier_plankton', 'batch_size': 16, 'min_epochs': 1, 'max_epochs': 5, 'learning_rate': 0.002, 'deterministic_trainer': False, 'random_seed': 42, 'debug_mode': False, 'fast_dev_run': False, 'precision': 32, 'plugins': None, 'accelerator': None, 'num_workers': 20, 'gpus': 1, 'num_nodes': 1, 'log_interval': 10, 'use_weighted_loss': False, 'use_pretrained': False, 'plankton_data_base_path': '/gpfs/work/machnitz/plankton_dataset', 'new_sorted_plankton_data': 'new_data/4David/M160/Sorted', 'new_unsorted_plankton_data': 'new_data/4David/HE570/rois/20*/20*', 'old_sorted_plankton_data': 'VPR_M87_grouped', 'use_old_data': False, 'use_new_data': True, 'super_classes': None, 'use_subclasses': False, 'preload_dataset': False, 'excluded_classes': ['Blurry', 'Bubbles'], 'train_split': 0.7, 'validation_split': 0.2, 'log_confusion_matrices': True, 'log_images': False, 'shuffle_train_dataset': True, 'shuffle_validation_dataset': False, 'shuffle_test_dataset': False, 'tensorboard_logger_logdir': '../logs/tb_logs/', 'checkpoint_file_path': 'logs/checkpoints', 'final_image_size': 128}\n",
      "Load new data: 100%|██████████| 21/21 [00:01<00:00, 17.24it/s]\n",
      "/gpfs/home/greenber/anaconda3/envs/plankton/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(CONFIG.random_seed)\n",
    "pl.seed_everything(CONFIG.random_seed)\n",
    "\n",
    "if CONFIG.debug_mode:\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(name)s %(funcName)s %(levelname)s %(message)s')\n",
    "else:\n",
    "    logging.basicConfig(level=logging.WARNING, format='%(name)s %(funcName)s %(levelname)s %(message)s')\n",
    "\n",
    "if CONFIG.debug_mode:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "logging.warning(CONFIG.__dict__)  # prints the whole config used for that run\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(CONFIG.final_image_size),\n",
    "    transforms.CenterCrop([CONFIG.final_image_size, CONFIG.final_image_size]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_module = PlanktonDataLoader.from_argparse_args(CONFIG, transform=transform)\n",
    "data_module.setup()\n",
    "\n",
    "for batch in data_module.train_dataloader():\n",
    "    example_input, _, _ = batch\n",
    "    break\n",
    "\n",
    "# if the model is trained on GPU add a GPU logger to see GPU utilization in comet-ml logs:\n",
    "if CONFIG.gpus == 0:\n",
    "    callbacks = None\n",
    "else:\n",
    "    callbacks = [pl.callbacks.GPUStatsMonitor()]\n",
    "\n",
    "# logging to tensorboard:\n",
    "experiment_name = f\"{CONFIG.experiment_name}_{dt.now().strftime('%d%m%YT%H%M%S')}\"\n",
    "test_tube_logger = pl_loggers.TestTubeLogger(save_dir=CONFIG.tensorboard_logger_logdir,\n",
    "                                             name=experiment_name,\n",
    "                                             create_git_tag=False,\n",
    "                                             log_graph=True)\n",
    "\n",
    "# initializes a callback to save the 5 best model weights measured by the lowest loss:\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"NLL Validation\",\n",
    "                                      save_top_k=5,\n",
    "                                      mode='min',\n",
    "                                      save_last=True,\n",
    "                                      dirpath=os.path.join(CONFIG.checkpoint_file_path, experiment_name),\n",
    "                                      )\n",
    "\n",
    "model = LightningModel(class_labels=data_module.unique_labels,\n",
    "                       all_labels=data_module.all_labels,\n",
    "                       example_input_array=example_input,\n",
    "                       **CONFIG.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9c0451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "CONFIG.max_epochs = max_epochs\n",
    "trainer = pl.Trainer.from_argparse_args(CONFIG,\n",
    "                                        callbacks=callbacks,\n",
    "                                        logger=[test_tube_logger],\n",
    "                                        checkpoint_callback=checkpoint_callback,\n",
    "                                        log_every_n_steps=CONFIG.log_interval,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698c6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13635 training, 3896 validation, 1948 testing images\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(data_module.train_data.files)} training, {len(data_module.valid_data.files)} validation, {len(data_module.test_data.files)} testing images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49cc433d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name          | Type       | Params | In sizes          | Out sizes\n",
      "-----------------------------------------------------------------------------\n",
      "0 | model         | Sequential | 25.6 M | [16, 3, 128, 128] | [16, 17] \n",
      "1 | loss_func     | NLLLoss    | 0      | ?                 | ?        \n",
      "2 | accuracy_func | Accuracy   | 0      | ?                 | ?        \n",
      "-----------------------------------------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.296   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3b8d11092a43769c508ee8703ed19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plankton",
   "language": "python",
   "name": "plankton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
